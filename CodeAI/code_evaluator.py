"""
CodeBLEU Evaluation Tool
Evaluates generated code correctness using CodeBLEU metric
"""

import re
from typing import List, Tuple, Optional, Dict
import subprocess
import sys
import os


class CodeBLEUEvaluator:
    """
    Evaluates code correctness using CodeBLEU metric.
    CodeBLEU combines BLEU score with code-specific metrics.
    """
    
    def __init__(self):
        """Initialize the CodeBLEU evaluator."""
        self._ensure_dependencies()
    
    def _ensure_dependencies(self):
        """Ensure required dependencies are installed."""
        try:
            import nltk
            # Try to download punkt if not available
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt', quiet=True)
        except ImportError:
            print("Warning: nltk not found. Some features may be limited.")
            print("Install with: pip install nltk")
        
        # tree-sitter is optional for enhanced AST matching
        try:
            import tree_sitter
        except ImportError:
            # tree-sitter is optional, will use fallback methods
            pass
    
    def evaluate(self, generated_code: str, reference_code: str, language: str = "python") -> Dict[str, float]:
        """
        Evaluate generated code against reference code using CodeBLEU.
        
        Args:
            generated_code: The code generated by the AI agent
            reference_code: The correct/reference code to compare against
            language: Programming language of the code
        
        Returns:
            Dictionary containing CodeBLEU score and component scores
        """
        # Normalize code (remove whitespace differences)
        gen_normalized = self._normalize_code(generated_code)
        ref_normalized = self._normalize_code(reference_code)
        
        # Calculate BLEU score (n-gram overlap)
        bleu_score = self._calculate_bleu(gen_normalized, ref_normalized)
        
        # Calculate code-specific metrics
        syntax_match = self._syntax_match_score(gen_normalized, ref_normalized, language)
        dataflow_match = self._dataflow_match_score(gen_normalized, ref_normalized, language)
        ast_match = self._ast_match_score(gen_normalized, ref_normalized, language)
        
        # Calculate CodeBLEU (weighted combination)
        codebleu_score = (
            0.25 * bleu_score +
            0.25 * syntax_match +
            0.25 * dataflow_match +
            0.25 * ast_match
        )
        
        return {
            "codebleu": codebleu_score,
            "bleu": bleu_score,
            "syntax_match": syntax_match,
            "dataflow_match": dataflow_match,
            "ast_match": ast_match,
            "is_correct": codebleu_score >= 0.75  # Threshold for correctness
        }
    
    def _normalize_code(self, code: str) -> str:
        """Normalize code by removing extra whitespace and comments."""
        # Remove comments
        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)
        code = re.sub(r'//.*$', '', code, flags=re.MULTILINE)
        code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
        
        # Normalize whitespace
        lines = [line.strip() for line in code.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def _calculate_bleu(self, generated: str, reference: str) -> float:
        """
        Calculate BLEU score between generated and reference code.
        Simplified version focusing on token overlap.
        """
        try:
            from nltk.translate.bleu_score import sentence_bleu
            from nltk.tokenize import word_tokenize
            
            gen_tokens = word_tokenize(generated.lower())
            ref_tokens = word_tokenize(reference.lower())
            
            # Calculate BLEU-4
            score = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25, 0.25, 0.25, 0.25))
            return float(score)
        except:
            # Fallback: simple token overlap
            gen_tokens = set(generated.lower().split())
            ref_tokens = set(reference.lower().split())
            
            if not ref_tokens:
                return 0.0
            
            overlap = len(gen_tokens & ref_tokens)
            return overlap / len(ref_tokens) if ref_tokens else 0.0
    
    def _syntax_match_score(self, generated: str, reference: str, language: str) -> float:
        """
        Calculate syntax match score by comparing code structure.
        Simplified version that checks keyword and structure similarity.
        """
        # Extract keywords and structure elements
        gen_keywords = self._extract_keywords(generated, language)
        ref_keywords = self._extract_keywords(reference, language)
        
        if not ref_keywords:
            return 1.0 if not gen_keywords else 0.0
        
        overlap = len(gen_keywords & ref_keywords)
        return overlap / len(ref_keywords)
    
    def _extract_keywords(self, code: str, language: str) -> set:
        """Extract language-specific keywords from code."""
        # Common keywords across languages
        common_keywords = {
            'def', 'class', 'if', 'else', 'for', 'while', 'return', 'import',
            'public', 'private', 'static', 'void', 'int', 'string', 'bool',
            'function', 'var', 'let', 'const', 'async', 'await'
        }
        
        # Language-specific patterns
        if language == "python":
            keywords = {'def', 'class', 'if', 'elif', 'else', 'for', 'while', 
                       'return', 'import', 'from', 'try', 'except', 'with', 'as'}
        elif language in ["java", "cpp"]:
            keywords = {'public', 'private', 'protected', 'static', 'void', 'int', 
                       'string', 'class', 'if', 'else', 'for', 'while', 'return'}
        else:
            keywords = common_keywords
        
        code_lower = code.lower()
        found_keywords = {kw for kw in keywords if kw in code_lower}
        
        # Also extract function/class names
        if language == "python":
            func_pattern = r'def\s+(\w+)'
            class_pattern = r'class\s+(\w+)'
        elif language in ["java", "cpp"]:
            func_pattern = r'(\w+)\s*\([^)]*\)\s*\{'
            class_pattern = r'class\s+(\w+)'
        else:
            func_pattern = r'function\s+(\w+)'
            class_pattern = r'class\s+(\w+)'
        
        functions = set(re.findall(func_pattern, code))
        classes = set(re.findall(class_pattern, code))
        
        return found_keywords | functions | classes
    
    def _dataflow_match_score(self, generated: str, reference: str, language: str) -> float:
        """
        Calculate dataflow match score by comparing variable usage patterns.
        Simplified version.
        """
        gen_vars = self._extract_variables(generated, language)
        ref_vars = self._extract_variables(reference, language)
        
        if not ref_vars:
            return 1.0 if not gen_vars else 0.0
        
        overlap = len(gen_vars & ref_vars)
        return overlap / len(ref_vars) if ref_vars else 0.0
    
    def _extract_variables(self, code: str, language: str) -> set:
        """Extract variable names from code."""
        # Simple pattern matching for variable assignments
        if language == "python":
            pattern = r'(\w+)\s*='
        elif language in ["java", "cpp"]:
            pattern = r'(?:int|string|float|double|bool|char)\s+(\w+)\s*='
        else:
            pattern = r'(\w+)\s*='
        
        variables = set(re.findall(pattern, code))
        return variables
    
    def _ast_match_score(self, generated: str, reference: str, language: str) -> float:
        """
        Calculate AST (Abstract Syntax Tree) match score.
        Simplified version using tree-sitter if available, otherwise fallback.
        """
        try:
            from tree_sitter import Language, Parser
            
            # Try to use tree-sitter for AST comparison
            # This is a simplified version - full implementation would require
            # proper AST parsing and comparison
            return self._syntax_match_score(generated, reference, language)
        except:
            # Fallback to syntax match
            return self._syntax_match_score(generated, reference, language)
    
    def get_evaluation_report(self, generated_code: str, reference_code: str, 
                             language: str = "python") -> str:
        """
        Get a formatted evaluation report.
        
        Args:
            generated_code: Generated code
            reference_code: Reference code
            language: Programming language
        
        Returns:
            Formatted report string
        """
        results = self.evaluate(generated_code, reference_code, language)
        
        report = f"""
CodeBLEU Evaluation Report
==========================
Overall CodeBLEU Score: {results['codebleu']:.4f}
Correctness: {'âœ“ CORRECT' if results['is_correct'] else 'âœ— INCORRECT'}

Component Scores:
- BLEU Score: {results['bleu']:.4f}
- Syntax Match: {results['syntax_match']:.4f}
- Dataflow Match: {results['dataflow_match']:.4f}
- AST Match: {results['ast_match']:.4f}

Threshold: Code is considered correct if CodeBLEU >= 0.75
"""
        return report

